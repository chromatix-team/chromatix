{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Chromatix models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chromatix is a fully differentiable library, meaning we can calculate gradients w.r.t to (almost) every quantity in our models. In this notebook we'll show how to optimise and train Chromatix models using two of the most well-known optimisation libraries: [Optax](https://github.com/deepmind/optax) for deep-learning optimisers such as Adam, and [Jaxopt](https://github.com/google/jaxopt) for classical optimisers such as L-BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from chromatix.elements import ObjectivePointSource, FFLens, ZernikeAberrations\n",
    "from chromatix.utils import trainable\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making model and data\n",
    "\n",
    "As our model we'll take the example from the Zernike fitting, where we simulate a PSF with some Zernike coefficients, and try and infer them from this simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZernikePSF(nn.Module):\n",
    "    ansi_indices: np.ndarray = np.arange(1, 11)\n",
    "    camera_shape: Tuple[int, int] = (256, 256)\n",
    "    camera_pixel_pitch: float = 0.125\n",
    "    f: float = 100\n",
    "    NA: float = 0.8\n",
    "    n: float = 1.33\n",
    "    wavelength: float = 0.532\n",
    "    wavelength_ratio: float = 1.0\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self):\n",
    "        spacing = self.f * self.wavelength/ (self.n * self.camera_shape[0] * self.camera_pixel_pitch)\n",
    "        field = ObjectivePointSource(self.camera_shape, spacing, self.wavelength, self.wavelength_ratio, self.f, self.n, self.NA, power=1e7)(z=0)\n",
    "        field = ZernikeAberrations(trainable(jnp.zeros_like(self.ansi_indices, dtype=jnp.float32)), self.f, self.n, self.NA, self.ansi_indices)(field)\n",
    "        field = FFLens(self.f, self.n)(field)\n",
    "        return field "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the model, and simulate the data using some coefficients. We then define a loss function, which should return a (loss, metrics) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating model\n",
    "model = ZernikePSF()\n",
    "\n",
    "# Specify \"ground truth\" parameters for Zernike coefficients\n",
    "coefficients_truth = jnp.array([2.0, 5.0, 3.0, 0, 1, 0, 1, 0, 1, 0])\n",
    "params_true = jax.tree_map(lambda x: coefficients_truth, model.init(key)) # easiest to just do a tree_map\n",
    "\n",
    "# Generating data\n",
    "data = model.apply(params_true).intensity.squeeze()\n",
    "\n",
    "# Our loss function\n",
    "def loss_fn(params, data):\n",
    "    psf_estimate = model.apply(params).intensity.squeeze()\n",
    "    loss = jnp.mean((psf_estimate - data)**2) / jnp.mean(data**2)\n",
    "    return loss, {\"loss\": loss}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model and data, we infer the parameters by training the model using optax. We'll use the Adam optimiser (note the very high learning rate!) and use Flax's `TrainState` to deal with the optimiser state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the state which has the model, params and optimiser\n",
    "state = TrainState.create(apply_fn=model.apply, \n",
    "                          params=model.init(key), \n",
    "                          tx=optax.adam(learning_rate=0.5))\n",
    "\n",
    "# Defining the function which returns the gradients\n",
    "grad_fn = jax.jit(jax.grad(loss_fn, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'loss': Array(3.695996, dtype=float32)}\n",
      "100 {'loss': Array(0.06857503, dtype=float32)}\n",
      "200 {'loss': Array(0.02074304, dtype=float32)}\n",
      "300 {'loss': Array(6.8586576e-07, dtype=float32)}\n",
      "400 {'loss': Array(2.7676396e-11, dtype=float32)}\n",
      "CPU times: user 4.22 s, sys: 292 ms, total: 4.51 s\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Simple training loop\n",
    "max_iterations = 500\n",
    "for iteration in range(max_iterations):\n",
    "    grads, metrics = grad_fn(state.params, data) \n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(iteration, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned coefficients: [2. 5. 3. 0. 1. 0. 1. 0. 1. 0.]\n",
      "True Coefficients: [2. 5. 3. 0. 1. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learned coefficients: {jnp.abs(jnp.around(state.params['params']['ZernikeAberrations_0']['zernike_coefficients'], 2))}\")\n",
    "print(f\"True Coefficients: {coefficients_truth}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Jaxopt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of Jax's use of pytrees, classical optimisation using Jaxopt doesn't require any code change! We can optimise this model using the following simple two-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining solver\n",
    "solver = jaxopt.LBFGS(loss_fn, has_aux=True)\n",
    "\n",
    "# Running solver\n",
    "res = solver.run(model.init(key), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned coefficients: [2.   5.   3.   0.   1.   0.   1.   0.   1.01 0.01]\n",
      "True Coefficients: [2. 5. 3. 0. 1. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learned coefficients: {jnp.abs(jnp.around(res.params['params']['ZernikeAberrations_0']['zernike_coefficients'], 2))}\")\n",
    "print(f\"True Coefficients: {coefficients_truth}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
