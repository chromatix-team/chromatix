{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Chromatix models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chromatix is a fully differentiable library, meaning we can calculate gradients w.r.t to (almost) every quantity in our models. In this notebook we'll show how to optimise and train Chromatix models using two of the most well-known optimisation libraries: [Optax](https://github.com/deepmind/optax) for deep-learning optimisers such as Adam, and [Jaxopt](https://github.com/google/jaxopt) for classical optimisers such as L-BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax.linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "import numpy as np\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from chromatix.elements import ObjectivePointSource, FFLens, ZernikeAberrations, trainable\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making model and data\n",
    "\n",
    "As our model we'll take the example from the Zernike fitting, where we simulate a PSF with some Zernike coefficients, and try and infer them from this simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZernikePSF(nn.Module):\n",
    "    ansi_indices: np.ndarray = np.arange(1, 11)\n",
    "    camera_shape: Tuple[int, int] = (256, 256)\n",
    "    camera_pixel_pitch: float = 0.125\n",
    "    f: float = 100\n",
    "    NA: float = 0.8\n",
    "    n: float = 1.33\n",
    "    wavelength: float = 0.532\n",
    "    wavelength_ratio: float = 1.0\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self):\n",
    "        spacing = self.f * self.wavelength/ (self.n * self.camera_shape[0] * self.camera_pixel_pitch)\n",
    "        field = ObjectivePointSource(self.camera_shape, spacing, self.wavelength, self.wavelength_ratio, self.f, self.n, self.NA, power=1e7)(z=0)\n",
    "        field = ZernikeAberrations(trainable(jnp.zeros_like(self.ansi_indices, dtype=jnp.float32)), self.f, self.n, self.NA, self.ansi_indices)(field)\n",
    "        field = FFLens(self.f, self.n)(field)\n",
    "        return field "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialising the model, we get a dictionary consisting of both trainable parameters and a so-called state. The state contains all things we want calculated once and want to cache. Here it's just some of the other parameters, but it can also be a more complicated phasemask or a propagator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    state: {\n",
      "        ObjectivePointSource_0: {\n",
      "            _f: 100,\n",
      "            _n: 1.33,\n",
      "            _NA: 0.8,\n",
      "            _power: 10000000.0,\n",
      "            _amplitude: 1.0,\n",
      "        },\n",
      "        FFLens_0: {\n",
      "            _f: 100,\n",
      "            _n: 1.33,\n",
      "            _NA: None,\n",
      "        },\n",
      "    },\n",
      "    params: {\n",
      "        ZernikeAberrations_0: {\n",
      "            _coefficients: Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
      "        },\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "model = ZernikePSF()\n",
    "variables = model.init(key)\n",
    "print(variables)\n",
    "\n",
    "# Split into two\n",
    "params, state = variables[\"params\"], variables[\"state\"]\n",
    "del variables # delete for memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some synthetic data data using some coefficients. Note that the loss function has two inputs\n",
    "\n",
    " We then define a loss function, which should return a (loss, metrics) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify \"ground truth\" parameters for Zernike coefficients\n",
    "coefficients_truth = jnp.array([2.0, 5.0, 3.0, 0, 1, 0, 1, 0, 1, 0])\n",
    "params_true = unfreeze(params)\n",
    "params_true[\"ZernikeAberrations_0\"][\"_coefficients\"] = coefficients_truth\n",
    "params_true = freeze(params_true)\n",
    "\n",
    "# Generating data\n",
    "data = model.apply({\"params\": params_true, \"state\": state}).intensity.squeeze()\n",
    "\n",
    "# Our loss function\n",
    "def loss_fn(params, state, data):\n",
    "    psf_estimate = model.apply({\"params\": params, \"state\": state}).intensity.squeeze()\n",
    "    loss = jnp.mean((psf_estimate - data)**2) / jnp.mean(data**2)\n",
    "    return loss, {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model and data, we infer the parameters by training the model using optax. We'll use the Adam optimiser (note the very high learning rate!) and use Flax's `TrainState` to deal with the optimiser state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the state which has the model, params and optimiser\n",
    "trainstate = TrainState.create(apply_fn=model.apply, \n",
    "                          params=params, \n",
    "                          tx=optax.adam(learning_rate=0.5))\n",
    "\n",
    "# Defining the function which returns the gradients\n",
    "grad_fn = jax.jit(jax.grad(loss_fn, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'loss': Array(3.695996, dtype=float32)}\n",
      "100 {'loss': Array(0.06857503, dtype=float32)}\n",
      "200 {'loss': Array(0.02074304, dtype=float32)}\n",
      "300 {'loss': Array(6.8586576e-07, dtype=float32)}\n",
      "400 {'loss': Array(2.7676396e-11, dtype=float32)}\n",
      "CPU times: user 4.09 s, sys: 239 ms, total: 4.33 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Simple training loop\n",
    "max_iterations = 500\n",
    "for iteration in range(max_iterations):\n",
    "    grads, metrics = grad_fn(trainstate.params, state, data) \n",
    "    trainstate = trainstate.apply_gradients(grads=grads)\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(iteration, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned coefficients: [2. 5. 3. 0. 1. 0. 1. 0. 1. 0.]\n",
      "True Coefficients: [2. 5. 3. 0. 1. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learned coefficients: {jnp.abs(jnp.around(trainstate.params['ZernikeAberrations_0']['_coefficients'], 2))}\")\n",
    "print(f\"True Coefficients: {coefficients_truth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of Jax's use of pytrees, classical optimisation using Jaxopt doesn't require any code change! We can optimise this model using the following simple two-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining solver\n",
    "solver = jaxopt.LBFGS(loss_fn, has_aux=True)\n",
    "\n",
    "# Running solver\n",
    "res = solver.run(model.init(key)[\"params\"], state, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned coefficients: [2.   5.   3.   0.   1.   0.   1.   0.   1.01 0.01]\n",
      "True Coefficients: [2. 5. 3. 0. 1. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learned coefficients: {jnp.abs(jnp.around(res.params['ZernikeAberrations_0']['_coefficients'], 2))}\")\n",
    "print(f\"True Coefficients: {coefficients_truth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
