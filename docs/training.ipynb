{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Chromatix models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/chromatix-team/chromatix/blob/main/docs/training.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in Colab, install Chromatix. Don't forget to select a GPU!\n",
    "!pip install --upgrade pip\n",
    "!pip install git+https://github.com/chromatix-team/chromatix.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chromatix is a fully differentiable library, meaning we can calculate gradients with respect to (almost) every quantity in our models. In this notebook we'll show how to optimize and train Chromatix models using the most well-known JAX optimization library: [Optax](https://github.com/deepmind/optax) for deep learning optimizers such as Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import skimage\n",
    "from einops import rearrange\n",
    "from jaxtyping import Array\n",
    "\n",
    "from chromatix import Field\n",
    "from chromatix.elements import (\n",
    "    FFLens,\n",
    "    PhaseMask,\n",
    "    PlaneWave,\n",
    "    Propagate,\n",
    ")\n",
    "from chromatix.functional import (\n",
    "    amplitude_change,\n",
    "    ff_lens,\n",
    "    phase_change,\n",
    "    plane_wave,\n",
    "    transfer_propagate,\n",
    ")\n",
    "from chromatix.systems import OpticalSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Optax\n",
    "\n",
    "Most of the time, you'll want to optimize simulations in Chromatix using a gradient descent optimizer or more generally a modern deep-learning gradient descent optimizer like Adam (especially when you have millions of parameters, e.g. if your parameters are pixels in a sample). These first-order gradient descent methods are available via a library called [Optax](https://optax.readthedocs.io/en/latest/). There are three ways you might go about organizing optimizations like that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Optimizing parameters of a simulation using the functional API\n",
    "\n",
    "The simplest and most flexible way to define a simulation and optimize it is to use the functional API. For most quick experiments or computational optics inverse problems, this is the method you will want to use. This involves writing a function that takes the optical parameter(s) you wish to optimize as an input and then using `jax.grad` to differentiate a loss with respect to that parameter. Here we show an example of this style for performing [Fourier ptychography via gradient descent](https://chromatix.readthedocs.io/en/latest/examples/fourier_ptychography/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 loss = 1.8397162959704616e-10 over 121 images\n",
      "iteration 2 loss = 6.692484674081234e-12 over 121 images\n",
      "iteration 3 loss = 8.807251260754823e-13 over 121 images\n",
      "iteration 4 loss = 7.475818466877449e-13 over 121 images\n",
      "iteration 5 loss = 6.836539255712648e-13 over 121 images\n",
      "iteration 6 loss = 6.424953884892615e-13 over 121 images\n",
      "iteration 7 loss = 6.116519508762852e-13 over 121 images\n",
      "iteration 8 loss = 5.859798865755217e-13 over 121 images\n",
      "iteration 9 loss = 5.633953590114538e-13 over 121 images\n",
      "iteration 10 loss = 5.431229467903198e-13 over 121 images\n"
     ]
    }
   ],
   "source": [
    "# First we define the simulation we want to optimize as a function.\n",
    "# In this case, we simulate imaging a sample illuminated at an angle\n",
    "# using a low NA microscope.\n",
    "def tilted_illumination_system(amplitude: Array, phase: Array, kykx: Array) -> Array:\n",
    "    field = plane_wave(amplitude.shape, 0.3, 0.532, kykx=kykx)\n",
    "    field = amplitude_change(field, amplitude)\n",
    "    field = phase_change(field, phase)\n",
    "    field = ff_lens(field, 1.8e3, 1.33)\n",
    "    field = ff_lens(field, 1.8e3, 1.33, NA=0.3)\n",
    "    return field.intensity\n",
    "\n",
    "\n",
    "# Here's some data we are using to simulate some \"measurements\"\n",
    "# and then perform a reconstruction.\n",
    "amplitude = skimage.data.camera().astype(\"float\")\n",
    "amplitude = amplitude / amplitude.max()\n",
    "phase = skimage.data.moon().astype(\"float\")\n",
    "phase = np.pi * phase / phase.max()\n",
    "\n",
    "\n",
    "# Now we simulate some \"measurements\" from our low NA imaging system.\n",
    "kykx = (\n",
    "    jnp.array(\n",
    "        jnp.meshgrid(jnp.linspace(-0.5, 0.5, num=11), jnp.linspace(-0.5, 0.5, num=11))\n",
    "    )\n",
    "    * 2\n",
    "    * jnp.pi\n",
    ")\n",
    "kykx = rearrange(kykx, \"d h w -> (h w) d\")\n",
    "images = jax.vmap(\n",
    "    lambda k: tilted_illumination_system(jnp.array(amplitude), jnp.array(phase), k)\n",
    ")(kykx)\n",
    "\n",
    "\n",
    "# We're initializing the parameters that we want to optimize, i.e. the amplitude and phase of the sample.\n",
    "parameters = (images[60][::-1, ::-1], jnp.zeros_like(images[60]))\n",
    "# We also initialize an optimizer (in this case we're just performing gradient descent, so there's no optimizer state).\n",
    "optimizer = optax.sgd(1e13)\n",
    "opt_state = optimizer.init(parameters)\n",
    "\n",
    "\n",
    "# This defines our loss function, and it takes in the parameters\n",
    "# we want to take gradients with respect to as the first argument.\n",
    "def fp_loss_fn(parameters: Array, measured_image: Array, kykx: Array) -> Array:\n",
    "    # 1. Extract the amplitude and phase from the parameters tuple.\n",
    "    # Remember that the first element of the tuple is the amplitude and\n",
    "    # the second element is the phase.\n",
    "    amplitude = parameters[0]\n",
    "    phase = parameters[1]\n",
    "    # 2. Simulate imaging the amplitude and phase you just got using\n",
    "    # the forward model we defined previously.\n",
    "    simulated = tilted_illumination_system(amplitude, phase, kykx)\n",
    "    # 3. Return the mean squared error between the simulated image of\n",
    "    # our reconstruction and the \"measured\" image that we passed to\n",
    "    # this loss function. WARNING: Make sure to squeeze the output of your\n",
    "    # simulation! Otherwise you'll get weird broadcasting that creates a\n",
    "    # huge array, giving you incorrect results and making things really slow.\n",
    "    return jnp.mean((simulated - measured_image) ** 2)\n",
    "\n",
    "\n",
    "# This defines the update step which computes the loss but also the gradient of the loss\n",
    "# with respect to our parameters (i.e. our guess of the reconstructed sample). We then use\n",
    "# that gradient and our optimizer to update our reconstruction.\n",
    "@jax.jit\n",
    "def update(\n",
    "    parameters: tuple[Array, Array], opt_state: Any, image: Array, kykx: Array\n",
    ") -> tuple[tuple[Array, Array], Any]:\n",
    "    loss, grads = jax.value_and_grad(fp_loss_fn)(parameters, image, kykx)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    parameters = optax.apply_updates(parameters, updates)\n",
    "    return loss, parameters, opt_state\n",
    "\n",
    "\n",
    "# We then run this update step multiple times over all 121 measured images in order to arrive at a reconstruction.\n",
    "# We've chosen the learning rate for you so that this should appropriately converge if everything's gone right.\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    for j in range(kykx.shape[0]):\n",
    "        loss, parameters, opt_state = update(parameters, opt_state, images[j], kykx[j])\n",
    "        losses.append(np.array(loss))\n",
    "    print(\n",
    "        f\"iteration {i + 1} loss = {np.mean(np.array(losses[-1 : -kykx.shape[0] : -1]))} over {kykx.shape[0]} images\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Combining an optical simulation with its parameters and data using custom Equinox Modules\n",
    "\n",
    "Sometimes, you might want to wrap up a simulation, its optimizable parameters, and also any other parameters that define the simulation together. This is useful for keeping track of the proper parameters in a convenient way without having to pass them around explicitly all the time, serializing/saving different simulation configurations, and when you want to optimize multiple kinds of parameters at the same time. We use Equinox [`Module`s](https://docs.kidger.site/equinox/api/module/module/) to do this. **Note**: the only two real changes to the style of the optimization here are how we define the simulation and how we pass the parameters to the loss function (we pass the whole `Module` rather than just the parameter itself). Here we'll show a simple example of this style using our [holography example](https://chromatix.readthedocs.io/en/latest/examples/cgh/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:51: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'correlation': Array(0.00036571, dtype=float32), 'loss': Array(0.99963427, dtype=float32)}\n",
      "200 {'correlation': Array(0.70244604, dtype=float32), 'loss': Array(0.29755396, dtype=float32)}\n",
      "400 {'correlation': Array(0.70941615, dtype=float32), 'loss': Array(0.29058385, dtype=float32)}\n",
      "600 {'correlation': Array(0.7137658, dtype=float32), 'loss': Array(0.2862342, dtype=float32)}\n",
      "800 {'correlation': Array(0.7153402, dtype=float32), 'loss': Array(0.2846598, dtype=float32)}\n",
      "CPU times: user 31 s, sys: 489 ms, total: 31.5 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "class CGH(eqx.Module):\n",
    "    phase: (\n",
    "        Array  # This is the phase mask we want to optimize, and is not marked static!\n",
    "    )\n",
    "    shape: tuple[int, int] = eqx.field(static=True)\n",
    "    spacing: float = eqx.field(static=True)\n",
    "    z: Array = eqx.field(static=True)\n",
    "    f: float = eqx.field(static=True)\n",
    "    n: float = eqx.field(static=True)\n",
    "    NA: float | None = eqx.field(static=True)\n",
    "    pad_width: int = eqx.field(static=True)\n",
    "    spectrum: float | Array = eqx.field(static=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape: tuple[int, int],\n",
    "        spacing: float,  # microns\n",
    "        z: Array,  # microns\n",
    "        f: float = 200.0e3,  # microns\n",
    "        n: float = 1.0,\n",
    "        NA: float | None = None,\n",
    "        pad_width: int = 0,\n",
    "        spectrum: float | Array = 1.035,  # microns\n",
    "    ):\n",
    "        self.shape = shape\n",
    "        self.spacing = spacing\n",
    "        self.z = z\n",
    "        self.f = f\n",
    "        self.n = n\n",
    "        self.NA = NA\n",
    "        self.pad_width = pad_width\n",
    "        self.spectrum = spectrum\n",
    "        self.phase = jnp.zeros(\n",
    "            self.shape\n",
    "        )  # Initialization of our phase mask parameter to zeros\n",
    "\n",
    "    def __call__(self) -> Field:\n",
    "        field = plane_wave(self.shape, self.spacing, self.spectrum)\n",
    "        field = phase_change(field, self.phase)\n",
    "        field = ff_lens(field, self.f, self.n, self.NA)\n",
    "        field = transfer_propagate(\n",
    "            field, self.z, self.n, pad_width=self.pad_width, mode=\"same\"\n",
    "        )\n",
    "        return field\n",
    "\n",
    "\n",
    "# Let's initialize the holography model\n",
    "shape = (256, 256)\n",
    "spacing = 9.2  # microns\n",
    "z = jnp.linspace(0.0, 100.0e4, num=51)  # Planes we want to simulate the hologram at\n",
    "model = CGH(shape=shape, spacing=spacing, z=z)\n",
    "\n",
    "\n",
    "# Now we create the optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-1)\n",
    "opt_state = optimizer.init(model)\n",
    "\n",
    "\n",
    "# Create a target pattern for which we want to optimize a hologram\n",
    "sample = np.zeros((51, 256, 256))\n",
    "sample[30, 128, 128] = 1.0\n",
    "sample[10, 51, 92] = 1.0\n",
    "sample[50, 10, 25] = 1.0\n",
    "diameter = 25\n",
    "kernel = np.zeros((diameter, diameter, diameter))\n",
    "grid = np.meshgrid(\n",
    "    np.linspace(-diameter / 2, diameter / 2, num=diameter),\n",
    "    np.linspace(-diameter / 2, diameter / 2, num=diameter),\n",
    "    np.linspace(-diameter / 2, diameter / 2, num=diameter),\n",
    ")\n",
    "grid = np.sqrt(grid[0] ** 2 + grid[1] ** 2 + grid[2] ** 2)\n",
    "kernel[grid < diameter / 5] = 1.0\n",
    "sample = jnp.fft.ifftn(\n",
    "    jnp.fft.fftn(jnp.array(sample)) * jnp.fft.fftn(jnp.array(kernel), s=sample.shape)\n",
    ").real\n",
    "sample = sample[..., jnp.newaxis, jnp.newaxis]\n",
    "sample *= 1000.0\n",
    "\n",
    "\n",
    "# Here we define a loss function, but note that this time\n",
    "# we're taking the whole model directly! The model itself\n",
    "# contains the parameters, and we'll update the model itself\n",
    "# in our optimization loop.\n",
    "def cgh_loss_fn(model, target):\n",
    "    approx = model().intensity\n",
    "    correlation = jnp.corrcoef(approx.flatten(), target.flatten())[0, 1]\n",
    "    loss = 1.0 - correlation\n",
    "    return loss, {\"loss\": loss, \"correlation\": correlation}\n",
    "\n",
    "\n",
    "# This is our update function just like before, but this time\n",
    "# taking the model directly.\n",
    "@jax.jit\n",
    "def update(model, opt_state, target):\n",
    "    grads, metrics = jax.grad(cgh_loss_fn, has_aux=True)(model, target)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = optax.apply_updates(model, updates)\n",
    "    return model, opt_state, metrics\n",
    "\n",
    "\n",
    "# Now we just run the optimization loop!\n",
    "max_iterations = 1000\n",
    "history = {\n",
    "    \"loss\": np.zeros((max_iterations)),\n",
    "    \"correlation\": np.zeros((max_iterations)),\n",
    "}\n",
    "for iteration in range(max_iterations):\n",
    "    model, opt_state, metrics = update(model, opt_state, sample)\n",
    "    for m in metrics:\n",
    "        history[m][iteration] = metrics[m]\n",
    "    if iteration % 200 == 0:\n",
    "        print(iteration, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using OpticalSystem and Equinox partitioning to choose what parameters are optimizable\n",
    "\n",
    "The previous approaches have defined the parameter to be optimized either by setting that parameter as the first argument of the function we differentiate or by creating an Equinox `Module` that has all its attributes set to static except the parameter we wish to optimize. You'll notice that Equinox prints out a warning due to this use of static because it can lead to confusing bugs when combined with other JAX transformations. We set everything that's not optimized to static in order to easily avoid having to deal with gradients to wrong parameters in our model. However, this abuse of static could be limiting (e.g. if we need to combine the optical simulation with other JAX transformations like `vmap` or optimize only some parameters and not others with the same simulation). Another way to choose what parameters of a simulation are optimizable is with `partition`. Here, we'll revisit the CGH example and create an `OpticalSystem` in a very succinct way which does not let us choose what is optimizable. Then, we'll use `partition` to select which parameter should be optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'correlation': Array(0.00036571, dtype=float32), 'loss': Array(0.99963427, dtype=float32)}\n",
      "200 {'correlation': Array(0.70244604, dtype=float32), 'loss': Array(0.29755396, dtype=float32)}\n",
      "400 {'correlation': Array(0.70941615, dtype=float32), 'loss': Array(0.29058385, dtype=float32)}\n",
      "600 {'correlation': Array(0.7137658, dtype=float32), 'loss': Array(0.2862342, dtype=float32)}\n",
      "800 {'correlation': Array(0.7153402, dtype=float32), 'loss': Array(0.2846598, dtype=float32)}\n",
      "CPU times: user 29.2 s, sys: 577 ms, total: 29.8 s\n",
      "Wall time: 54.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's initialize the holography model\n",
    "shape = (256, 256)\n",
    "spacing = 9.2  # microns\n",
    "image_plane_spacing = 200.0e3 * 1.035 / (spacing * shape[0])\n",
    "z = jnp.linspace(0.0, 100.0e4, num=51)  # Planes we want to simulate the hologram at\n",
    "# This time, the model doesn't let us set static fields\n",
    "# because we are not creating our own Module.\n",
    "model = OpticalSystem(\n",
    "    [\n",
    "        PlaneWave(shape, spacing, spectrum=1.035),\n",
    "        PhaseMask(phase=jnp.zeros(shape)),\n",
    "        FFLens(f=200.0e3, n=1.0),\n",
    "        Propagate(\n",
    "            Field.empty(shape, image_plane_spacing, spectrum=1.035),\n",
    "            z,\n",
    "            n=1.0,\n",
    "            method=\"transfer\",\n",
    "            mode=\"same\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# We want to create the optimizer, but first we have to\n",
    "# select which parameter to optimize. We start by setting\n",
    "# that we don't want to optimize any parameter:\n",
    "filter_spec = jax.tree.map(lambda _: False, model)\n",
    "# Then, we select the parameter we actually want to optimize.\n",
    "# In this line, the lambda function selects the phase pixels\n",
    "# from the second element in our optical system, and sets that to true:\n",
    "filter_spec = eqx.tree_at(lambda m: m.elements[1].phase, filter_spec, True)\n",
    "# Then we split the model using this filter_spec into the part that\n",
    "# we want to optimize (the parameters) and the part we don't (the state):\n",
    "parameters, state = eqx.partition(model, filter_spec)\n",
    "# Now we can create the optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-1)\n",
    "opt_state = optimizer.init(parameters)\n",
    "\n",
    "\n",
    "# Create a target pattern for which we want to optimize a hologram\n",
    "sample = np.zeros((51, 256, 256))\n",
    "sample[30, 128, 128] = 1.0\n",
    "sample[10, 51, 92] = 1.0\n",
    "sample[50, 10, 25] = 1.0\n",
    "diameter = 25\n",
    "kernel = np.zeros((diameter, diameter, diameter))\n",
    "grid = np.meshgrid(\n",
    "    np.linspace(-diameter / 2, diameter / 2, num=diameter),\n",
    "    np.linspace(-diameter / 2, diameter / 2, num=diameter),\n",
    "    np.linspace(-diameter / 2, diameter / 2, num=diameter),\n",
    ")\n",
    "grid = np.sqrt(grid[0] ** 2 + grid[1] ** 2 + grid[2] ** 2)\n",
    "kernel[grid < diameter / 5] = 1.0\n",
    "sample = jnp.fft.ifftn(\n",
    "    jnp.fft.fftn(jnp.array(sample)) * jnp.fft.fftn(jnp.array(kernel), s=sample.shape)\n",
    ").real\n",
    "sample = sample[..., jnp.newaxis, jnp.newaxis]\n",
    "sample *= 1000.0\n",
    "\n",
    "\n",
    "# Here we define a loss function, but note that this time\n",
    "# we're taking the parameters and state separately! The model\n",
    "# is reconstructed in the loss function, and we'll update just\n",
    "# the parameters in our optimization loop. The overhead of this\n",
    "# recombination is compiled away by JAX.\n",
    "def cgh_loss_fn_partitioned(parameters, state, target):\n",
    "    model = eqx.combine(parameters, state)\n",
    "    approx = model().intensity\n",
    "    correlation = jnp.corrcoef(approx.flatten(), target.flatten())[0, 1]\n",
    "    loss = 1.0 - correlation\n",
    "    return loss, {\"loss\": loss, \"correlation\": correlation}\n",
    "\n",
    "\n",
    "# This is our update function just like before, but this time\n",
    "# we partition and combine the model before computing the loss.\n",
    "@jax.jit\n",
    "def update(model, opt_state, target):\n",
    "    parameters, state = eqx.partition(model, filter_spec)\n",
    "    grads, metrics = jax.grad(cgh_loss_fn_partitioned, has_aux=True)(\n",
    "        parameters, state, target\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, parameters)\n",
    "    parameters = optax.apply_updates(parameters, updates)\n",
    "    model = eqx.combine(parameters, state)\n",
    "    return model, opt_state, metrics\n",
    "\n",
    "\n",
    "# Now we just run the optimization loop!\n",
    "max_iterations = 1000\n",
    "history = {\n",
    "    \"loss\": np.zeros((max_iterations)),\n",
    "    \"correlation\": np.zeros((max_iterations)),\n",
    "}\n",
    "for iteration in range(max_iterations):\n",
    "    model, opt_state, metrics = update(model, opt_state, sample)\n",
    "    for m in metrics:\n",
    "        history[m][iteration] = metrics[m]\n",
    "    if iteration % 200 == 0:\n",
    "        print(iteration, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
