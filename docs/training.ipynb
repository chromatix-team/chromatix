{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Chromatix models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to easily train chromatix models with either optax for deep-learning style optimisers such as Adam or Jaxopt for more classical optimisation such as conjugate gradient.\n",
    "\n",
    "As our model we'll take the Zernike polynomials tutorial. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "\n",
    "import optax\n",
    "import jaxopt\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from chromatix.elements import ObjectivePointSource, FFLens, ZernikeAberrations\n",
    "from chromatix.utils import trainable\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model\n",
    "camera_shape: Tuple[int, int] = (256, 256)\n",
    "camera_pixel_pitch: float = 0.125\n",
    "f: float = 100\n",
    "NA: float = 0.8\n",
    "n: float = 1.33\n",
    "wavelength: float = 0.532\n",
    "wavelength_ratio: float = 1.0\n",
    "spacing = f * wavelength/ (n * camera_shape[0] * camera_pixel_pitch)\n",
    "\n",
    "class ZernikePSF(nn.Module):\n",
    "    ansi_indices: np.ndarray\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self):\n",
    "        field = ObjectivePointSource(camera_shape, spacing, wavelength, wavelength_ratio, f, n, NA, power=1e7)(z=0)\n",
    "        field = ZernikeAberrations(trainable(jnp.zeros_like(self.ansi_indices, dtype=jnp.float32)), f, n, NA, self.ansi_indices)(field)\n",
    "        field = FFLens(f, n)(field)\n",
    "        return field "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the model, and simulate the data using some coefficients. We then define a loss function, which should return a (loss, metrics) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating model\n",
    "model = ZernikePSF(np.arange(1, 11))\n",
    "params = model.init(key)\n",
    "\n",
    "# Specify \"ground truth\" parameters for Zernike coefficients\n",
    "coefficients_truth = jnp.array([2.0, 5.0, 3.0, 0, 1, 0, 1, 0, 1, 0])\n",
    "params_true = jax.tree_map(lambda x: coefficients_truth, params) # easiest to just do a tree_map\n",
    "\n",
    "# Generating data\n",
    "data = model.apply(params_true).intensity.squeeze()\n",
    "\n",
    "# Our loss function\n",
    "def loss_fn(params, data):\n",
    "    psf_estimate = model.apply(params).intensity.squeeze()\n",
    "    loss = jnp.mean((psf_estimate - data)**2) / jnp.mean(data**2)\n",
    "    return loss, {\"loss\": loss}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train using optax. We use flax's TrainState to handle the optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up training: optimiser, state and grad_fn\n",
    "optimizer = optax.adam(learning_rate=0.5)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "grad_fn = jax.jit(jax.grad(loss_fn, has_aux=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'loss': Array(3.695996, dtype=float32)}\n",
      "100 {'loss': Array(0.06857503, dtype=float32)}\n",
      "200 {'loss': Array(0.02074304, dtype=float32)}\n",
      "300 {'loss': Array(6.8586576e-07, dtype=float32)}\n",
      "400 {'loss': Array(2.7676396e-11, dtype=float32)}\n",
      "CPU times: user 4.11 s, sys: 268 ms, total: 4.38 s\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Simple training loop\n",
    "max_iterations = 500\n",
    "for iteration in range(max_iterations):\n",
    "    grads, metrics = grad_fn(state.params, data) \n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(iteration, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        ZernikeAberrations_0: {\n",
      "            zernike_coefficients: Array([ 2.0000017e+00,  5.0000029e+00, -3.0000019e+00, -2.2701904e-06,\n",
      "                   -1.0000006e+00,  7.8149048e-07,  1.0000002e+00,  1.0779938e-06,\n",
      "                    9.9999923e-01,  6.9686160e-07], dtype=float32),\n",
      "        },\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(state.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Jaxopt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jax ecosystem also has a rich set of classical optimisers such as LBFGS or Conjugate gradient solvers - all implemented in Jaxopt. Here we show how to train the model we defined above using Jaxopt. Because of Jax' use of pytrees, everything composes nicely and you don't need to make any change to your models to use Jaxopt instead of Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(key) # reset the parameters\n",
    "\n",
    "# Defining solver\n",
    "solver = jaxopt.LBFGS(loss_fn, has_aux=True)\n",
    "\n",
    "# Running solver\n",
    "res = solver.run(params, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        ZernikeAberrations_0: {\n",
      "            zernike_coefficients: Array([ 1.9957651e+00,  4.9954557e+00,  2.9984114e+00, -1.5044229e-03,\n",
      "                    1.0011595e+00, -1.2449678e-03,  9.9792808e-01, -2.7636252e-03,\n",
      "                    1.0069935e+00,  5.0141285e-03], dtype=float32),\n",
      "        },\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(res.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
